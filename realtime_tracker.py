#!/usr/bin/env python3
"""
å®æ—¶å·¥å…·è·Ÿè¸ªæ£€æµ‹ç³»ç»Ÿ
æ”¯æŒåŠ¨æ€ç›®æ ‡è·Ÿè¸ªå’Œå®æ—¶æ£€æµ‹æ¡†æ¸²æŸ“
"""
import cv2
import time
import json
import threading
from datetime import datetime
from pathlib import Path
from production_tool_detector import ProductionToolDetector, SystemConfig
import numpy as np
import torch
from PIL import Image
import open_clip

class RealTimeTracker:
    """å®æ—¶å·¥å…·è·Ÿè¸ªå™¨"""
    
    def __init__(self, config: SystemConfig, detection_interval=1.0):
        """
        åˆå§‹åŒ–å®æ—¶è·Ÿè¸ªå™¨
        
        Args:
            config: ç³»ç»Ÿé…ç½®
            detection_interval: æ£€æµ‹é—´éš”ï¼ˆç§’ï¼‰
        """
        self.config = config
        self.detection_interval = detection_interval
        self.last_detection_time = 0
        self.detected_objects = []  # å­˜å‚¨æ£€æµ‹åˆ°çš„å¯¹è±¡ä½ç½®å’ŒçŠ¶æ€
        self.is_detecting = False
        
        # åˆå§‹åŒ–CLIPæ¨¡å‹
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, _, self.preprocess = open_clip.create_model_and_transforms(
            config.model_name, pretrained="openai"
        )
        self.model = self.model.to(self.device)
        self.tokenizer = open_clip.get_tokenizer(config.model_name)
        
        # å·¥å…·ç±»åˆ«å’Œæ¨¡æ¿
        self.tool_categories = [
            "hammer", "pliers", "wrench", "flat_screwdriver", 
            "cross_screwdriver", "cutter", "tape_measure", "hex_key_set", "screw_box"
        ]
        
        # ä¸ºæ¯ä¸ªå·¥å…·åˆ›å»ºæ–‡æœ¬æ¨¡æ¿
        self.tool_templates = {}
        for tool in self.tool_categories:
            self.tool_templates[tool] = {
                "positive": [tool, f"a {tool}", f"{tool} tool"],
                "negative": ["empty space", "background", "nothing", "absent"]
            }
    
    def sliding_window_detection(self, frame, window_size=200, stride=100):
        """ä½¿ç”¨æ»‘åŠ¨çª—å£åœ¨å¸§ä¸­æ£€æµ‹å·¥å…·"""
        height, width = frame.shape[:2]
        detections = []
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        for y in range(0, height - window_size, stride):
            for x in range(0, width - window_size, stride):
                # æå–çª—å£
                window = frame_rgb[y:y+window_size, x:x+window_size]
                window_pil = Image.fromarray(window)
                
                # é¢„å¤„ç†
                image_input = self.preprocess(window_pil).unsqueeze(0).to(self.device)
                
                # å¯¹æ¯ä¸ªå·¥å…·ç±»åˆ«è¿›è¡Œæ£€æµ‹
                best_tool = None
                best_confidence = -1.0
                
                for tool_name in self.tool_categories:
                    templates = self.tool_templates[tool_name]
                    all_texts = templates["positive"] + templates["negative"]
                    text_tokens = self.tokenizer(all_texts).to(self.device)
                    
                    with torch.no_grad():
                        image_features = self.model.encode_image(image_input)
                        text_features = self.model.encode_text(text_tokens)
                        
                        # å½’ä¸€åŒ–
                        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                        
                        # è®¡ç®—ç›¸ä¼¼åº¦
                        similarities = (image_features @ text_features.T).softmax(dim=-1)
                        
                        # åˆ†æç»“æœ
                        pos_count = len(templates["positive"])
                        positive_scores = similarities[0][:pos_count]
                        negative_scores = similarities[0][pos_count:]
                        
                        avg_positive = positive_scores.mean().item()
                        avg_negative = negative_scores.mean().item()
                        confidence = avg_positive - avg_negative
                        
                        # åªä¿ç•™é«˜ç½®ä¿¡åº¦çš„æ£€æµ‹
                        if confidence > self.config.confidence_threshold and confidence > best_confidence:
                            best_confidence = confidence
                            best_tool = tool_name
                
                # å¦‚æœæ£€æµ‹åˆ°å·¥å…·ï¼Œè®°å½•ä½ç½®
                if best_tool and best_confidence > 0.002:  # æé«˜é˜ˆå€¼é¿å…è¯¯æ£€
                    detections.append({
                        'tool_name': best_tool,
                        'confidence': best_confidence,
                        'bbox': [x, y, window_size, window_size],
                        'center': [x + window_size//2, y + window_size//2]
                    })
        
        return detections
    
    def non_max_suppression(self, detections, iou_threshold=0.3):
        """éæå¤§å€¼æŠ‘åˆ¶ï¼Œå»é™¤é‡å çš„æ£€æµ‹æ¡†"""
        if not detections:
            return []
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
        
        final_detections = []
        for detection in detections:
            bbox1 = detection['bbox']
            should_keep = True
            
            for kept_detection in final_detections:
                bbox2 = kept_detection['bbox']
                
                # è®¡ç®—IoU
                x1_inter = max(bbox1[0], bbox2[0])
                y1_inter = max(bbox1[1], bbox2[1])
                x2_inter = min(bbox1[0] + bbox1[2], bbox2[0] + bbox2[2])
                y2_inter = min(bbox1[1] + bbox1[3], bbox2[1] + bbox2[3])
                
                if x2_inter > x1_inter and y2_inter > y1_inter:
                    intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)
                    area1 = bbox1[2] * bbox1[3]
                    area2 = bbox2[2] * bbox2[3]
                    union = area1 + area2 - intersection
                    
                    if intersection / union > iou_threshold:
                        should_keep = False
                        break
            
            if should_keep:
                final_detections.append(detection)
        
        return final_detections
    
    def detect_frame_async(self, frame):
        """å¼‚æ­¥æ£€æµ‹å¸§ä¸­çš„å·¥å…·"""
        if self.is_detecting:
            return
            
        current_time = time.time()
        if current_time - self.last_detection_time < self.detection_interval:
            return
            
        self.is_detecting = True
        self.last_detection_time = current_time
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œæ£€æµ‹
        detection_thread = threading.Thread(
            target=self._detect_frame_worker,
            args=(frame.copy(),)
        )
        detection_thread.daemon = True
        detection_thread.start()
    
    def _detect_frame_worker(self, frame):
        """æ£€æµ‹å·¥ä½œçº¿ç¨‹"""
        try:
            # ç¼©æ”¾å¸§ä»¥æé«˜æ£€æµ‹é€Ÿåº¦
            scale_factor = 0.5
            small_frame = cv2.resize(frame, None, fx=scale_factor, fy=scale_factor)
            
            # æ»‘åŠ¨çª—å£æ£€æµ‹
            detections = self.sliding_window_detection(small_frame, window_size=100, stride=50)
            
            # å°†åæ ‡ç¼©æ”¾å›åŸå§‹å°ºå¯¸
            for det in detections:
                det['bbox'] = [int(x / scale_factor) for x in det['bbox']]
                det['center'] = [int(x / scale_factor) for x in det['center']]
            
            # éæå¤§å€¼æŠ‘åˆ¶
            self.detected_objects = self.non_max_suppression(detections, iou_threshold=0.3)
            
        except Exception as e:
            print(f"âŒ æ£€æµ‹é”™è¯¯: {e}")
        finally:
            self.is_detecting = False
    
    def draw_tracking_overlay(self, frame):
        """ç»˜åˆ¶è·Ÿè¸ªè¦†ç›–å±‚"""
        overlay = frame.copy()
        height, width = frame.shape[:2]
        
        # ç»˜åˆ¶æ£€æµ‹åˆ°çš„å·¥å…·
        for detection in self.detected_objects:
            x, y, w, h = detection['bbox']
            tool_name = detection['tool_name']
            confidence = detection['confidence']
            
            # æ ¹æ®ç½®ä¿¡åº¦é€‰æ‹©é¢œè‰²
            if confidence > 0.005:
                color = (0, 255, 0)  # ç»¿è‰² - é«˜ç½®ä¿¡åº¦
            elif confidence > 0.002:
                color = (0, 255, 255)  # é»„è‰² - ä¸­ç­‰ç½®ä¿¡åº¦
            else:
                color = (255, 0, 0)  # è“è‰² - ä½ç½®ä¿¡åº¦
            
            # ç»˜åˆ¶æ£€æµ‹æ¡†
            cv2.rectangle(overlay, (x, y), (x + w, y + h), color, 2)
            
            # ç»˜åˆ¶å·¥å…·åç§°å’Œç½®ä¿¡åº¦
            label = f"{tool_name}: {confidence:.3f}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            
            # èƒŒæ™¯çŸ©å½¢
            cv2.rectangle(overlay, (x, y - 25), (x + label_size[0] + 10, y), color, -1)
            
            # æ–‡æœ¬
            cv2.putText(overlay, label, (x + 5, y - 8), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # ç»˜åˆ¶çŠ¶æ€æ 
        status_bg = np.zeros((100, width, 3), dtype=np.uint8)
        cv2.rectangle(status_bg, (0, 0), (width, 100), (50, 50, 50), -1)
        
        # ç»Ÿè®¡ä¿¡æ¯
        tool_count = len(self.detected_objects)
        status_text = f"Real-time Tracking | Detected Objects: {tool_count}"
        cv2.putText(overlay, status_text, (20, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # æ£€æµ‹çŠ¶æ€
        status_color = (0, 255, 255) if self.is_detecting else (0, 255, 0)
        status_indicator = "DETECTING..." if self.is_detecting else "READY"
        cv2.putText(overlay, status_indicator, (20, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)
        
        # æ··åˆçŠ¶æ€æ 
        overlay[:100] = cv2.addWeighted(overlay[:100], 0.7, status_bg, 0.3, 0)
        
        return overlay
    
    def run_camera_tracking(self, camera_id=0):
        """è¿è¡Œæ‘„åƒå¤´å®æ—¶è·Ÿè¸ª"""
        print(f"ğŸ¥ å¯åŠ¨å®æ—¶å·¥å…·è·Ÿè¸ª (è®¾å¤‡ID: {camera_id})")
        print("æŒ‰ 'q' é€€å‡º, æŒ‰ 's' ä¿å­˜å½“å‰å¸§, æŒ‰ 'r' æ‰‹åŠ¨è§¦å‘æ£€æµ‹")
        
        cap = cv2.VideoCapture(camera_id)
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_id}")
            return
        
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´ç”»é¢")
                break
            
            # å¼‚æ­¥æ£€æµ‹
            self.detect_frame_async(frame)
            
            # ç»˜åˆ¶è·Ÿè¸ªè¦†ç›–å±‚
            display_frame = self.draw_tracking_overlay(frame)
            
            # æ˜¾ç¤ºç”»é¢
            cv2.imshow('Real-time Tool Tracking', display_frame)
            
            # å¤„ç†æŒ‰é”®
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('s'):
                # ä¿å­˜å½“å‰å¸§
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"tracking_capture_{timestamp}.jpg"
                cv2.imwrite(filename, display_frame)
                print(f"ğŸ“¸ å·²ä¿å­˜: {filename}")
            elif key == ord('r'):
                # æ‰‹åŠ¨è§¦å‘æ£€æµ‹
                self.last_detection_time = 0
                print("ğŸ”„ æ‰‹åŠ¨è§¦å‘æ£€æµ‹...")
        
        cap.release()
        cv2.destroyAllWindows()
        print("ğŸ¥ å®æ—¶è·Ÿè¸ªå·²åœæ­¢")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("""
ğŸ¯ å®æ—¶å·¥å…·è·Ÿè¸ªç³»ç»Ÿ

ä½¿ç”¨æ–¹æ³•:
  python realtime_tracker.py camera [camera_id]     # æ‘„åƒå¤´å®æ—¶è·Ÿè¸ª (é»˜è®¤ID=0)

æ§åˆ¶é”®:
  q - é€€å‡º
  s - ä¿å­˜å½“å‰å¸§
  r - æ‰‹åŠ¨è§¦å‘æ£€æµ‹

ç‰¹æ€§:
  â€¢ å®æ—¶ç›®æ ‡æ£€æµ‹å’Œè·Ÿè¸ª
  â€¢ åŠ¨æ€æ£€æµ‹æ¡†è·Ÿéšå·¥å…·ç§»åŠ¨
  â€¢ æ»‘åŠ¨çª—å£æ‰«ææ•´ä¸ªç”»é¢
  â€¢ éæå¤§å€¼æŠ‘åˆ¶å»é™¤é‡å¤æ£€æµ‹
  â€¢ ç½®ä¿¡åº¦é¢œè‰²ç¼–ç 

ç¤ºä¾‹:
  python realtime_tracker.py camera 0
        """)
        return
    
    command = sys.argv[1].lower()
    
    if command != "camera":
        print("âŒ å½“å‰åªæ”¯æŒæ‘„åƒå¤´æ¨¡å¼")
        return
    
    # é…ç½®æ£€æµ‹ç³»ç»Ÿ
    config = SystemConfig(
        confidence_threshold=0.0005,
        uncertainty_threshold=-0.0001,
        save_roi_images=False,
        log_level="ERROR"
    )
    
    camera_id = int(sys.argv[2]) if len(sys.argv) > 2 else 0
    tracker = RealTimeTracker(config, detection_interval=0.8)  # æ›´é¢‘ç¹çš„æ£€æµ‹
    
    try:
        tracker.run_camera_tracking(camera_id)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç³»ç»Ÿ")
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")

if __name__ == "__main__":
    main()