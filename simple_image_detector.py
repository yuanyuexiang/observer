#!/usr/bin/env python3
"""
ç®€å•å›¾ç‰‡å·¥å…·æ£€æµ‹å™¨ - çº¯é™æ€å›¾ç‰‡æ£€æµ‹ï¼Œæ— è§†é¢‘è·Ÿè¸ª
æ”¯æŒå•å¼ å›¾ç‰‡æ£€æµ‹å’Œæ‰¹é‡æ£€æµ‹
"""
import cv2
import torch
from PIL import Image
import open_clip
import numpy as np
import os
import glob
from datetime import datetime
import argparse

class SimpleImageDetector:
    """ç®€å•å›¾ç‰‡å·¥å…·æ£€æµ‹å™¨"""
    
    def __init__(self):
        # åˆå§‹åŒ–CLIPæ¨¡å‹
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ åˆå§‹åŒ–CLIPæ¨¡å‹ (è®¾å¤‡: {self.device})")
        
        self.model, _, self.preprocess = open_clip.create_model_and_transforms(
            "ViT-B-32", pretrained="openai"
        )
        self.model = self.model.to(self.device)
        self.tokenizer = open_clip.get_tokenizer("ViT-B-32")
        
        # å·¥å…·ç±»åˆ«å®šä¹‰
        self.tool_descriptions = [
            "a metal hammer tool",
            "a screwdriver with handle", 
            "metal pliers for gripping",
            "a wrench for bolts",
            "a measuring tape tool",
            "a cutting tool knife",
            "a hex allen key tool",
            "a toolbox with screws"
        ]
        
        self.tool_names = [
            "hammer", "screwdriver", "pliers", "wrench", 
            "tape measure", "cutter", "hex key", "screw box"
        ]
        
        print("âœ… å›¾ç‰‡æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def detect_tools_in_image(self, image_path, save_result=True):
        """æ£€æµ‹å•å¼ å›¾ç‰‡ä¸­çš„å·¥å…·"""
        print(f"\nğŸ” åˆ†æå›¾ç‰‡: {image_path}")
        
        try:
            # åŠ è½½å›¾ç‰‡
            image = cv2.imread(image_path)
            if image is None:
                print(f"âŒ æ— æ³•åŠ è½½å›¾ç‰‡: {image_path}")
                return None
            
            # è½¬æ¢ä¸ºRGBå¹¶é¢„å¤„ç†
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image_pil = Image.fromarray(image_rgb)
            image_input = self.preprocess(image_pil).unsqueeze(0).to(self.device)
            
            # CLIPæ¨ç†
            text_tokens = self.tokenizer(self.tool_descriptions).to(self.device)
            
            with torch.no_grad():
                image_features = self.model.encode_image(image_input)
                text_features = self.model.encode_text(text_tokens)
                
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                similarities = (image_features @ text_features.T).softmax(dim=-1)[0]
            
            # åˆ†æç»“æœ
            results = []
            for i, (tool_name, description, score) in enumerate(zip(self.tool_names, self.tool_descriptions, similarities)):
                results.append({
                    'tool': tool_name,
                    'description': description,
                    'score': score.item(),
                    'rank': i + 1
                })
            
            # æŒ‰åˆ†æ•°æ’åº
            results.sort(key=lambda x: x['score'], reverse=True)
            
            # æ˜¾ç¤ºç»“æœ
            print(f"ğŸ“Š æ£€æµ‹ç»“æœ (æŒ‰ç½®ä¿¡åº¦æ’åº):")
            for i, result in enumerate(results):
                icon = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
                print(f"{icon} {result['tool']:12} - {result['score']:.4f} ({result['description']})")
            
            # ç¡®å®šæœ€å¯èƒ½çš„å·¥å…·
            best_result = results[0]
            print(f"\nğŸ¯ æœ€å¯èƒ½çš„å·¥å…·: {best_result['tool'].upper()} (ç½®ä¿¡åº¦: {best_result['score']:.4f})")
            
            # ä¿å­˜å¸¦æ ‡æ³¨çš„ç»“æœå›¾ç‰‡
            if save_result:
                self.save_annotated_image(image, image_path, best_result, results[:3])
            
            return {
                'image_path': image_path,
                'best_tool': best_result,
                'all_results': results,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âŒ å¤„ç†é”™è¯¯: {e}")
            return None
    
    def save_annotated_image(self, image, original_path, best_result, top_results):
        """ä¿å­˜å¸¦æ ‡æ³¨çš„ç»“æœå›¾ç‰‡"""
        try:
            annotated = image.copy()
            height, width = image.shape[:2]
            
            # ç»˜åˆ¶ç»“æœä¿¡æ¯
            # ä¸»æ ‡é¢˜
            main_text = f"æ£€æµ‹ç»“æœ: {best_result['tool'].upper()}"
            font_scale = min(width / 800, 1.5)
            thickness = max(int(font_scale * 2), 2)
            
            # ä¸»æ ‡é¢˜èƒŒæ™¯
            (text_width, text_height), _ = cv2.getTextSize(main_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
            cv2.rectangle(annotated, (20, 20), (20 + text_width + 20, 20 + text_height + 20), (0, 255, 0), -1)
            cv2.putText(annotated, main_text, (30, 20 + text_height), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)
            
            # ç½®ä¿¡åº¦
            conf_text = f"ç½®ä¿¡åº¦: {best_result['score']:.3f}"
            cv2.putText(annotated, conf_text, (30, 20 + text_height + 40), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale * 0.6, (255, 255, 255), max(thickness - 1, 1))
            
            # å³ä¾§æ˜¾ç¤ºå‰3åç»“æœ
            y_start = 100
            cv2.rectangle(annotated, (width - 250, y_start - 20), (width - 10, y_start + 120), (40, 40, 40), -1)
            cv2.putText(annotated, "TOP 3:", (width - 240, y_start), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            for i, result in enumerate(top_results):
                rank_text = f"{i+1}. {result['tool']}: {result['score']:.3f}"
                color = (0, 255, 0) if i == 0 else (255, 255, 255)
                cv2.putText(annotated, rank_text, (width - 240, y_start + 30 + i * 25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            
            # ä¿å­˜æ ‡æ³¨å›¾ç‰‡
            base_name = os.path.splitext(os.path.basename(original_path))[0]
            output_path = f"{base_name}_detected.jpg"
            cv2.imwrite(output_path, annotated)
            print(f"ğŸ’¾ å·²ä¿å­˜æ ‡æ³¨å›¾ç‰‡: {output_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ ‡æ³¨å›¾ç‰‡å¤±è´¥: {e}")
    
    def batch_detect(self, folder_path, pattern="*.jpg"):
        """æ‰¹é‡æ£€æµ‹æ–‡ä»¶å¤¹ä¸­çš„å›¾ç‰‡"""
        print(f"\nğŸ“ æ‰¹é‡æ£€æµ‹æ–‡ä»¶å¤¹: {folder_path}")
        print(f"ğŸ” æœç´¢æ¨¡å¼: {pattern}")
        
        # æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶
        search_pattern = os.path.join(folder_path, pattern)
        image_files = glob.glob(search_pattern)
        
        if not image_files:
            print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„å›¾ç‰‡æ–‡ä»¶: {search_pattern}")
            return []
        
        print(f"ğŸ“¸ æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")
        
        results = []
        for i, image_file in enumerate(image_files, 1):
            print(f"\n[{i}/{len(image_files)}] å¤„ç†: {os.path.basename(image_file)}")
            result = self.detect_tools_in_image(image_file)
            if result:
                results.append(result)
        
        # æ±‡æ€»æŠ¥å‘Š
        print(f"\nğŸ“‹ æ‰¹é‡æ£€æµ‹å®ŒæˆæŠ¥å‘Š:")
        print(f"æ€»è®¡å¤„ç†: {len(image_files)} å¼ å›¾ç‰‡")
        print(f"æˆåŠŸæ£€æµ‹: {len(results)} å¼ ")
        
        if results:
            print(f"\nğŸ† æ£€æµ‹ç»“æœæ±‡æ€»:")
            tool_counts = {}
            for result in results:
                tool = result['best_tool']['tool']
                tool_counts[tool] = tool_counts.get(tool, 0) + 1
            
            for tool, count in sorted(tool_counts.items(), key=lambda x: x[1], reverse=True):
                print(f"  {tool}: {count} å¼ å›¾ç‰‡")
        
        return results
    
    def interactive_mode(self):
        """äº¤äº’å¼æ£€æµ‹æ¨¡å¼"""
        print("\nğŸ¯ è¿›å…¥äº¤äº’å¼æ£€æµ‹æ¨¡å¼")
        print("è¾“å…¥å›¾ç‰‡è·¯å¾„è¿›è¡Œæ£€æµ‹ï¼Œè¾“å…¥ 'quit' é€€å‡º")
        
        while True:
            try:
                user_input = input("\nğŸ“· è¯·è¾“å…¥å›¾ç‰‡è·¯å¾„ (æˆ– 'quit' é€€å‡º): ").strip()
                
                if user_input.lower() in ['quit', 'q', 'exit']:
                    print("ğŸ‘‹ é€€å‡ºäº¤äº’æ¨¡å¼")
                    break
                
                if not user_input:
                    continue
                
                if os.path.isfile(user_input):
                    self.detect_tools_in_image(user_input)
                elif os.path.isdir(user_input):
                    self.batch_detect(user_input)
                else:
                    print(f"âŒ æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {user_input}")
                    
            except KeyboardInterrupt:
                print("\nğŸ‘‹ é€€å‡ºäº¤äº’æ¨¡å¼")
                break
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")


def main():
    parser = argparse.ArgumentParser(description='ç®€å•å›¾ç‰‡å·¥å…·æ£€æµ‹å™¨')
    parser.add_argument('input', nargs='?', help='å›¾ç‰‡æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--batch', action='store_true', help='æ‰¹é‡å¤„ç†æ¨¡å¼')
    parser.add_argument('--pattern', default='*.jpg', help='æ‰¹é‡å¤„ç†æ—¶çš„æ–‡ä»¶æ¨¡å¼ (é»˜è®¤: *.jpg)')
    parser.add_argument('--interactive', '-i', action='store_true', help='äº¤äº’å¼æ¨¡å¼')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = SimpleImageDetector()
    
    if args.interactive:
        # äº¤äº’å¼æ¨¡å¼
        detector.interactive_mode()
    elif args.input:
        if os.path.isfile(args.input):
            # å•ä¸ªæ–‡ä»¶æ£€æµ‹
            detector.detect_tools_in_image(args.input)
        elif os.path.isdir(args.input):
            # æ–‡ä»¶å¤¹æ‰¹é‡æ£€æµ‹
            detector.batch_detect(args.input, args.pattern)
        else:
            print(f"âŒ æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {args.input}")
    else:
        # é»˜è®¤äº¤äº’æ¨¡å¼
        detector.interactive_mode()


if __name__ == "__main__":
    main()