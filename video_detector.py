#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†é¢‘æµå·¥å…·æ£€æµ‹å™¨ - æ”¯æŒè§†é¢‘æ–‡ä»¶å’Œå®æ—¶æ‘„åƒå¤´æ£€æµ‹
"""

import cv2
import os
import time
import threading
from datetime import datetime
from types import SimpleNamespace
import json

class VideoToolDetector:
    """è§†é¢‘æµå·¥å…·æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.is_running = False
        self.detection_interval = 10  # æ£€æµ‹é—´éš”ï¼ˆç§’ï¼‰
        self.frame_count = 0
        self.last_detection_time = 0
        
    def setup_detectors(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨ç»„ä»¶"""
        try:
            from production_tool_detector import ProductionToolDetector
            from professional_annotator import ProfessionalToolboxAnnotator
            
            # ä½¿ç”¨ä¸simple_cliç›¸åŒçš„é…ç½®æ–¹å¼
            import logging
            
            class Config:
                def __init__(self):
                    self.model_name = 'ViT-B-32'
                    self.clip_model = 'ViT-B-32'
                    self.clip_pretrained = 'openai'
                    self.device = 'cpu'
                    self.confidence_threshold = 0.0
                    self.log_level = 'ERROR'
                    self.save_roi_images = True
                    self.output_dir = '.'
            
            config = Config()
            
            self.detector = ProductionToolDetector(config)
            self.annotator = ProfessionalToolboxAnnotator()
            self.workspace_config = self.detector.load_workspace_configuration('instances_default.json')
            
            print("âœ… æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def save_frame(self, frame, prefix="frame"):
        """ä¿å­˜è§†é¢‘å¸§ä¸ºå›¾ç‰‡"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{prefix}_{timestamp}.jpg"
        cv2.imwrite(filename, frame)
        return filename
    
    def detect_frame(self, frame, frame_source="video"):
        """å¯¹å•å¸§è¿›è¡Œå·¥å…·æ£€æµ‹"""
        try:
            # ä¿å­˜å¸§ä¸ºä¸´æ—¶å›¾ç‰‡
            temp_image = self.save_frame(frame, f"temp_{frame_source}")
            
            print(f"\nğŸ” å¼€å§‹æ£€æµ‹å¸§: {temp_image}")
            print(f"ğŸ“… æ£€æµ‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            # è¿è¡Œå·¥å…·æ£€æµ‹
            results = self.detector.run_full_detection(temp_image, self.workspace_config)
            analysis = self.detector.analyze_workspace_status(results)
            
            # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
            print(f"\n=== å·¥å…·æ£€æµ‹ç»“æœ ===")
            missing_tools = []
            present_tools = []
            
            for result in results:
                status_icon = {'present': 'âœ…', 'missing': 'âŒ', 'uncertain': 'ğŸ¤”', 'error': 'âš ï¸'}.get(result.status, '?')
                print(f"{result.tool_name:15} {status_icon} {result.status:10} (ç½®ä¿¡åº¦: {result.confidence:+.4f})")
                
                if result.status == 'missing':
                    missing_tools.append(result.tool_name)
                elif result.status == 'present':
                    present_tools.append(result.tool_name)
            
            print(f"\n=== å·¥å…·ç®±çŠ¶æ€æ‘˜è¦ ===")
            print(f"æ€»å·¥å…·æ•°: {analysis['total_tools']}")
            print(f"åœ¨ä½å·¥å…·: {analysis['present_tools']} âœ…")
            print(f"ç¼ºå¤±å·¥å…·: {analysis['missing_tools']} âŒ")
            print(f"å®Œæ•´æ€§: {analysis['completeness_rate']:.1f}%")
            print(f"æ•´ä½“çŠ¶æ€: {analysis['overall_status']}")
            
            if missing_tools:
                print(f"\nâš ï¸  ç¼ºå¤±çš„å·¥å…·:")
                for tool in missing_tools:
                    print(f"   - {tool}")
            else:
                print(f"\nğŸ‰ æ‰€æœ‰å·¥å…·éƒ½åœ¨å·¥å…·ç®±ä¸­!")
            
            # ç”Ÿæˆä¸“ä¸šæŠ¥å‘Š
            try:
                output_path = self.annotator.create_professional_status_report(
                    temp_image, results, analysis['completeness_rate']
                )
                if output_path:
                    print(f"ğŸ“¸ å·²ç”Ÿæˆä¸“ä¸šæ£€æµ‹æŠ¥å‘Š: {output_path}")
            except Exception as e:
                print(f"âš ï¸ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_image):
                os.remove(temp_image)
                
            return analysis
            
        except Exception as e:
            print(f"âŒ å¸§æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    def process_video_file(self, video_path, max_frames=None):
        """å¤„ç†è§†é¢‘æ–‡ä»¶"""
        if not os.path.exists(video_path):
            print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return
        
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = frame_count / fps if fps > 0 else 0
        
        print(f"\nğŸ“¹ å¼€å§‹å¤„ç†è§†é¢‘æ–‡ä»¶: {video_path}")
        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {frame_count} å¸§, {fps:.2f} FPS, {duration:.2f} ç§’")
        print(f"ğŸ” æ£€æµ‹é—´éš”: æ¯ {self.detection_interval} ç§’")
        
        detection_count = 0
        processed_frames = 0
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                current_time = processed_frames / fps if fps > 0 else 0
                
                # æ¯10ç§’æ£€æµ‹ä¸€æ¬¡
                if current_time >= detection_count * self.detection_interval:
                    print(f"\n{'='*50}")
                    print(f"ğŸ“ æ£€æµ‹ç‚¹ {detection_count + 1}: {current_time:.1f}ç§’")
                    
                    analysis = self.detect_frame(frame, f"video_{detection_count+1}")
                    if analysis:
                        detection_count += 1
                        
                        # å¦‚æœè®¾ç½®äº†æœ€å¤§å¸§æ•°é™åˆ¶
                        if max_frames and detection_count >= max_frames:
                            print(f"\nğŸ è¾¾åˆ°æœ€å¤§æ£€æµ‹å¸§æ•°é™åˆ¶: {max_frames}")
                            break
                
                processed_frames += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if processed_frames % 100 == 0:
                    progress = (processed_frames / frame_count) * 100 if frame_count > 0 else 0
                    print(f"ğŸ“ˆ å¤„ç†è¿›åº¦: {progress:.1f}% ({processed_frames}/{frame_count})")
        
        finally:
            cap.release()
            print(f"\nğŸ‰ è§†é¢‘å¤„ç†å®Œæˆ!")
            print(f"ğŸ“Š æ€»è®¡æ£€æµ‹äº† {detection_count} ä¸ªæ—¶é—´ç‚¹")
    
    def process_camera_stream(self, camera_id=0):
        """å¤„ç†æ‘„åƒå¤´å®æ—¶æµ"""
        cap = cv2.VideoCapture(camera_id)
        
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´è®¾å¤‡: {camera_id}")
            return
        
        print(f"\nğŸ“· å¼€å§‹å®æ—¶ç›‘æ§æ‘„åƒå¤´: {camera_id}")
        print(f"ğŸ” æ£€æµ‹é—´éš”: æ¯ {self.detection_interval} ç§’")
        print(f"ğŸšª æŒ‰ 'q' é”®é€€å‡ºï¼ŒæŒ‰ 'd' é”®ç«‹å³æ£€æµ‹")
        
        self.is_running = True
        detection_count = 0
        
        try:
            while self.is_running:
                ret, frame = cap.read()
                if not ret:
                    print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                    break
                
                # æ˜¾ç¤ºå®æ—¶ç”»é¢
                cv2.imshow('å·¥å…·æ£€æµ‹å®æ—¶ç›‘æ§', frame)
                
                current_time = time.time()
                
                # å®šæ—¶æ£€æµ‹æˆ–æ‰‹åŠ¨è§¦å‘æ£€æµ‹
                should_detect = (current_time - self.last_detection_time >= self.detection_interval)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("\nğŸ‘‹ ç”¨æˆ·ä¸»åŠ¨é€€å‡º")
                    break
                elif key == ord('d'):
                    should_detect = True
                    print("\nğŸ” æ‰‹åŠ¨è§¦å‘æ£€æµ‹")
                
                if should_detect:
                    detection_count += 1
                    print(f"\n{'='*50}")
                    print(f"ğŸ“ å®æ—¶æ£€æµ‹ {detection_count}")
                    
                    analysis = self.detect_frame(frame, f"camera_{detection_count}")
                    self.last_detection_time = current_time
                    
                    # åœ¨çª—å£æ ‡é¢˜æ˜¾ç¤ºæœ€æ–°çŠ¶æ€
                    if analysis:
                        completeness = analysis['completeness_rate']
                        window_title = f"å·¥å…·æ£€æµ‹å®æ—¶ç›‘æ§ - å®Œæ•´æ€§: {completeness:.1f}%"
                        cv2.setWindowTitle('å·¥å…·æ£€æµ‹å®æ—¶ç›‘æ§', window_title)
        
        finally:
            self.is_running = False
            cap.release()
            cv2.destroyAllWindows()
            print(f"\nğŸ‰ å®æ—¶ç›‘æ§ç»“æŸ!")
            print(f"ğŸ“Š æ€»è®¡è¿›è¡Œäº† {detection_count} æ¬¡æ£€æµ‹")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è§†é¢‘æµå·¥å…·æ£€æµ‹å™¨")
    parser.add_argument('source', help='æ•°æ®æº: å›¾ç‰‡è·¯å¾„ã€è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼Œæˆ–æ‘„åƒå¤´ID(0,1,2...)')
    parser.add_argument('--interval', type=int, default=10, help='æ£€æµ‹é—´éš”ï¼ˆç§’ï¼Œé»˜è®¤10ç§’ï¼‰')
    parser.add_argument('--max-frames', type=int, help='æœ€å¤§æ£€æµ‹å¸§æ•°ï¼ˆä»…ç”¨äºè§†é¢‘æ–‡ä»¶ï¼‰')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    video_detector = VideoToolDetector()
    video_detector.detection_interval = args.interval
    
    if not video_detector.setup_detectors():
        return
    
    source = args.source
    
    # åˆ¤æ–­æ•°æ®æºç±»å‹
    if source.isdigit():
        # æ‘„åƒå¤´è®¾å¤‡
        camera_id = int(source)
        print(f"ğŸ¥ æ£€æµ‹æ¨¡å¼: å®æ—¶æ‘„åƒå¤´ (è®¾å¤‡ID: {camera_id})")
        video_detector.process_camera_stream(camera_id)
    
    elif os.path.isfile(source):
        # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm']
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        
        file_ext = os.path.splitext(source)[1].lower()
        
        if file_ext in video_extensions:
            print(f"ğŸ¬ æ£€æµ‹æ¨¡å¼: è§†é¢‘æ–‡ä»¶")
            video_detector.process_video_file(source, args.max_frames)
        elif file_ext in image_extensions:
            print(f"ğŸ–¼ï¸ æ£€æµ‹æ¨¡å¼: é™æ€å›¾ç‰‡")
            # å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œæ£€æµ‹
            frame = cv2.imread(source)
            if frame is not None:
                video_detector.detect_frame(frame, "image")
            else:
                print(f"âŒ æ— æ³•è¯»å–å›¾ç‰‡: {source}")
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
    
    else:
        print(f"âŒ æ•°æ®æºä¸å­˜åœ¨: {source}")

if __name__ == "__main__":
    main()